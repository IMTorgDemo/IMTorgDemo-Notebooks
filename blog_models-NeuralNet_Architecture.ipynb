{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architectures using PyTorch\n",
    "Date: 2019-11-19  \n",
    "Author: Jason Beach  \n",
    "Categories: DataScience, DeepLearning  \n",
    "Tags: pytorch, python, tag3  \n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using and Creating Simple Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the pretrained ResNet-18\n",
    "resnet = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(resnet.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = list(resnet.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 7, 7])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to finetune only the top layer of the model, set as below.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# Replace the top layer for finetuning.\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n",
    "\n",
    "# Forward pass.\n",
    "images = torch.randn(64, 3, 224, 224)\n",
    "outputs = resnet(images)\n",
    "print (outputs.size())     # (64, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load the entire model\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save and load only the model parameters (recommended)\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a simple perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tensors\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#build a computational graph\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#compute gradients\n",
    "y.backward()\n",
    "\n",
    "#print out the gradients.\n",
    "print(x.grad)    #x.grad = 2 \n",
    "print(w.grad)    #w.grad = 1 \n",
    "print(b.grad)    #b.grad = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x_data = Variable(torch.Tensor([[10.0], [9.0], [3.0], [2.0]] ))\n",
    "y_data = Variable(torch.Tensor([[90.0], [80.0], [50.0], [30.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)         #one indep var and makes one prediction for the Ŷ variable at a time   \n",
    "    def forward(self, x):                           #forward pass refers to the calculation process of the output data from the input\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "criterion = torch.nn.MSELoss(size_average=False)    #loss function is calculated from the target y_data and the prediction y_pred in order to update weights\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)    #SGD optimizer for the update of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-2.9363e+08]], requires_grad=True), Parameter containing:\n",
      "tensor([-36516880.], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[8.6319e+08]], requires_grad=True), Parameter containing:\n",
      "tensor([1.0735e+08], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[-2.5375e+09]], requires_grad=True), Parameter containing:\n",
      "tensor([-3.1557e+08], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "#use 20 single passes of training and weight updates\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    if (epoch < 3 | epoch > 18):\n",
    "        print( list(model.parameters()) )\n",
    "    # Forward pass\n",
    "    y_pred = model(x_data)    \n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_data)    \n",
    "    # Backward pass - learning and updating the weights\n",
    "    loss.backward()\n",
    "    optimizer.step()          #performs a parameter update based on the current gradient\n",
    "    optimizer.zero_grad()     #every time a variable is back-propagated through, the gradient will be accumulated instead of being replaced   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted Y value:  tensor(-9.0442e+10)\n"
     ]
    }
   ],
   "source": [
    "#make predictions\n",
    "new_x = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(new_x)\n",
    "print(\"predicted Y value: \", y_pred.data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update to logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "     def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)     \n",
    "     def forward(self, x):\n",
    "        y_pred = F.sigmoid(self.linear(x))          #<<< simple modification\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCELoss(size_average=True)     # used for binary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.9676]], requires_grad=True), Parameter containing:\n",
      "tensor([0.8016], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[-0.9676]], requires_grad=True), Parameter containing:\n",
      "tensor([0.8016], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[-0.9676]], requires_grad=True), Parameter containing:\n",
      "tensor([0.8016], requires_grad=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "#use 20 single passes of training and weight updates\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    if (epoch < 3 | epoch > 18):\n",
    "        print( list(model.parameters()) )\n",
    "    # Forward pass\n",
    "    y_pred = model(x_data)    \n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_data)    \n",
    "    # Backward pass - learning and updating the weights\n",
    "    loss.backward()\n",
    "    optimizer.step()          #performs a parameter update based on the current gradient\n",
    "    optimizer.zero_grad()     #every time a variable is back-propagated through, the gradient will be accumulated instead of being replaced   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update to simple Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        y_pred = self.relu(self.linear(x))          #<<< simple mod instead of heavyside\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron()\n",
    "criterion = torch.nn.MSELoss(size_average=False)    #loss function is calculated from the target y_data and the prediction y_pred in order to update weights\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)    #SGD optimizer for the update of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.8250]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1126], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[-0.8250]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1126], requires_grad=True)]\n",
      "[Parameter containing:\n",
      "tensor([[-0.8250]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1126], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "#use 20 single passes of training and weight updates\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    if (epoch < 3 | epoch > 18):\n",
    "        print( list(model.parameters()) )\n",
    "    # Forward pass\n",
    "    y_pred = model(x_data)    \n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, y_data)    \n",
    "    # Backward pass - learning and updating the weights\n",
    "    loss.backward()\n",
    "    optimizer.step()          #performs a parameter update based on the current gradient\n",
    "    optimizer.zero_grad()     #every time a variable is back-propagated through, the gradient will be accumulated instead of being replaced   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using feed-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.ones(1, requires_grad=True)    #if False then gradient will not be calculated automatically\n",
    "print(x.grad)                            #returns None bc scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 2\n",
    "z.backward()     # automatically calculates the gradient\n",
    "print(x.grad)    # ∂z/∂x = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update for a Feedforward Network\n",
    "\n",
    "[ref: blog](https://medium.com/biaslyai/pytorch-introduction-to-neural-network-feedforward-neural-network-model-e7231cff47cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "            self.sigmoid = torch.nn.Sigmoid()        \n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            relu = self.relu(hidden)\n",
    "            output = self.fc2(relu)\n",
    "            output = self.sigmoid(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "def blob_label(y, label, loc):\n",
    "    target = numpy.copy(y)\n",
    "    for l in loc:\n",
    "        target[y == l] = label\n",
    "        return target\n",
    "    \n",
    "x_train, y_train = make_blobs(n_samples=40, n_features=2, cluster_std=1.5, shuffle=True)\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.FloatTensor(blob_label(y_train, 0, [0]))\n",
    "y_train = torch.FloatTensor(blob_label(y_train, 1, [1,2,3]))\n",
    "x_test, y_test = make_blobs(n_samples=10, n_features=2, cluster_std=1.5, shuffle=True)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.FloatTensor(blob_label(y_test, 0, [0]))\n",
    "y_test = torch.FloatTensor(blob_label(y_test, 1, [1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Feedforward(2, 10)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 0.467517226934433\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "before_train = criterion(y_pred.squeeze(), y_test)\n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.17611254751682281\n",
      "Epoch 1: train loss: 0.1333077847957611\n",
      "Epoch 2: train loss: 0.0991164818406105\n",
      "Epoch 3: train loss: 0.07093816995620728\n",
      "Epoch 4: train loss: 0.04720110446214676\n",
      "Epoch 5: train loss: 0.02680397406220436\n",
      "Epoch 6: train loss: 0.008982661180198193\n",
      "Epoch 7: train loss: -0.006804706063121557\n",
      "Epoch 8: train loss: -0.020925888791680336\n",
      "Epoch 9: train loss: -0.03372335433959961\n",
      "Epoch 10: train loss: -0.0454268679022789\n",
      "Epoch 11: train loss: -0.056216198951005936\n",
      "Epoch 12: train loss: -0.06623609364032745\n",
      "Epoch 13: train loss: -0.07560508698225021\n",
      "Epoch 14: train loss: -0.0844065323472023\n",
      "Epoch 15: train loss: -0.0927170142531395\n",
      "Epoch 16: train loss: -0.10059820115566254\n",
      "Epoch 17: train loss: -0.10810460150241852\n",
      "Epoch 18: train loss: -0.11527948081493378\n",
      "Epoch 19: train loss: -0.122161865234375\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):    \n",
    "    optimizer.zero_grad()    # Forward pass\n",
    "    y_pred = model(x_train)    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_train)\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 0.38520193099975586\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "after_train = criterion(y_pred.squeeze(), y_test) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in file tensor/two_layer_net_numpy.py\n",
    "import numpy as np\n",
    "\n",
    "def run_sim_numpy():\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "    # Create random input and output data\n",
    "    x = np.random.randn(N, D_in)\n",
    "    y = np.random.randn(N, D_out)\n",
    "\n",
    "    # Randomly initialize weights\n",
    "    w1 = np.random.randn(D_in, H)\n",
    "    w2 = np.random.randn(H, D_out)\n",
    "\n",
    "    learning_rate = 1e-6\n",
    "    for t in range(500):\n",
    "        # Forward pass: compute predicted y\n",
    "        h = x.dot(w1)\n",
    "        h_relu = np.maximum(h, 0)\n",
    "        y_pred = h_relu.dot(w2)\n",
    "        \n",
    "        # Compute and print loss\n",
    "        loss = np.square(y_pred - y).sum()\n",
    "        if t>498:\n",
    "            print(t, loss)\n",
    "        \n",
    "        # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "        grad_y_pred = 2.0 * (y_pred - y)\n",
    "        grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "        grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "        grad_h = grad_h_relu.copy()\n",
    "        grad_h[h < 0] = 0\n",
    "        grad_w1 = x.T.dot(grad_h)\n",
    "        \n",
    "        # Update weights\n",
    "        w1 -= learning_rate * grad_w1\n",
    "        w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 3.417254510017826e-07\n",
      "499 6.80805478305819e-07\n",
      "499 4.4091770001221044e-05\n",
      "499 3.199749824397876e-05\n",
      "499 4.691894222750476e-07\n",
      "499 1.5314605068389221e-07\n",
      "499 1.2785897363290369e-05\n",
      "499 5.866297454492024e-05\n",
      "412 ms ± 13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_sim_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim_torch():\n",
    "    # Code in file tensor/two_layer_net_tensor.py\n",
    "    import torch\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    # device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "    # Create random input and output data\n",
    "    x = torch.randn(N, D_in, device=device)\n",
    "    y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "    # Randomly initialize weights\n",
    "    w1 = torch.randn(D_in, H, device=device)\n",
    "    w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "    learning_rate = 1e-6\n",
    "    for t in range(500):\n",
    "        # Forward pass: compute predicted y\n",
    "        h = x.mm(w1)\n",
    "        h_relu = h.clamp(min=0)\n",
    "        y_pred = h_relu.mm(w2)\n",
    "        \n",
    "        # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "        # of shape (); we can get its value as a Python number with loss.item().\n",
    "        loss = (y_pred - y).pow(2).sum()\n",
    "        if t>498:\n",
    "            print(t, loss.item())\n",
    "        \n",
    "        # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "        grad_y_pred = 2.0 * (y_pred - y)\n",
    "        grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "        grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "        grad_h = grad_h_relu.clone()\n",
    "        grad_h[h < 0] = 0\n",
    "        grad_w1 = x.t().mm(grad_h)\n",
    "        \n",
    "        # Update weights using gradient descent\n",
    "        w1 -= learning_rate * grad_w1\n",
    "        w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 1.6832038454595022e-05\n",
      "499 9.516641148366034e-05\n",
      "499 1.5797930245753378e-05\n",
      "499 4.5778524508932605e-05\n",
      "499 6.939189188415185e-05\n",
      "499 0.00011741339403670281\n",
      "499 0.00010538773494772613\n",
      "499 2.471236075507477e-05\n",
      "253 ms ± 17 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_sim_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim_torchgrad():\n",
    "    # Code in file autograd/two_layer_net_autograd.py\n",
    "    import torch\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    # device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "    # Create random Tensors to hold input and outputs\n",
    "    x = torch.randn(N, D_in, device=device)\n",
    "    y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "    # Create random Tensors for weights; setting requires_grad=True means that we\n",
    "    # want to compute gradients for these Tensors during the backward pass.\n",
    "    w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "    w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "    learning_rate = 1e-6\n",
    "    for t in range(500):\n",
    "        # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "        # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "        # PyTorch to build a computational graph, allowing automatic computation of\n",
    "        # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "        # don't need to keep references to intermediate values.\n",
    "        y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "        \n",
    "        # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "        # is a Python number giving its value.\n",
    "        loss = (y_pred - y).pow(2).sum()\n",
    "        if t>498:\n",
    "            print(t, loss.item())\n",
    "            \n",
    "        # Use autograd to compute the backward pass. This call will compute the\n",
    "        # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "        # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "        # of the loss with respect to w1 and w2 respectively.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights using gradient descent. For this step we just want to mutate\n",
    "        # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "        # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "        # to prevent PyTorch from building a computational graph for the updates\n",
    "        with torch.no_grad():\n",
    "            w1 -= learning_rate * w1.grad\n",
    "            w2 -= learning_rate * w2.grad\n",
    "            \n",
    "        # Manually zero the gradients after running the backward pass\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 4.717827687272802e-05\n",
      "499 7.954805914778262e-05\n",
      "499 2.859023334167432e-05\n",
      "499 5.056699228589423e-05\n",
      "499 8.617802814114839e-05\n",
      "499 0.000970246153883636\n",
      "499 5.565648098126985e-05\n",
      "499 8.797551708994433e-05\n",
      "977 ms ± 31.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_sim_torchgrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim_torchnn():\n",
    "    # Code in file nn/two_layer_net_nn.py\n",
    "    import torch\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    # device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "    # Create random Tensors to hold inputs and outputs\n",
    "    x = torch.randn(N, D_in, device=device)\n",
    "    y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "    # Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "    # is a Module which contains other Modules, and applies them in sequence to\n",
    "    # produce its output. Each Linear Module computes output from input using a\n",
    "    # linear function, and holds internal Tensors for its weight and bias.\n",
    "    # After constructing the model we use the .to() method to move it to the\n",
    "    # desired device.\n",
    "    model = torch.nn.Sequential(\n",
    "              torch.nn.Linear(D_in, H),\n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Linear(H, D_out),\n",
    "            ).to(device)\n",
    "\n",
    "    # The nn package also contains definitions of popular loss functions; in this\n",
    "    # case we will use Mean Squared Error (MSE) as our loss function. Setting\n",
    "    # reduction='sum' means that we are computing the *sum* of squared errors rather\n",
    "    # than the mean; this is for consistency with the examples above where we\n",
    "    # manually compute the loss, but in practice it is more common to use mean\n",
    "    # squared error as a loss by setting reduction='elementwise_mean'.\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    learning_rate = 1e-4\n",
    "    for t in range(500):\n",
    "        # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "        # override the __call__ operator so you can call them like functions. When\n",
    "        # doing so you pass a Tensor of input data to the Module and it produces\n",
    "        # a Tensor of output data.\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "        # values of y, and the loss function returns a Tensor containing the loss.\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        if t>498:\n",
    "            print(t, loss.item())\n",
    "        \n",
    "        # Zero the gradients before running the backward pass.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "        # parameters of the model. Internally, the parameters of each Module are stored\n",
    "        # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "        # all learnable parameters in the model.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "        # we can access its data and gradients like we did before.\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param.data -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 1.4216263934940798e-06\n",
      "499 1.6250106682491605e-06\n",
      "499 1.296600567002315e-05\n",
      "499 7.413837010972202e-06\n",
      "499 7.071726940921508e-06\n",
      "499 1.4960786529627512e-06\n",
      "499 3.874725280184066e-06\n",
      "499 6.084675987949595e-06\n",
      "1.14 s ± 44.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_sim_torchnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim_torchoptim():\n",
    "    # Code in file nn/two_layer_net_optim.py\n",
    "    import torch\n",
    "\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "    # Create random Tensors to hold inputs and outputs.\n",
    "    x = torch.randn(N, D_in)\n",
    "    y = torch.randn(N, D_out)\n",
    "\n",
    "    # Use the nn package to define our model and loss function.\n",
    "    model = torch.nn.Sequential(\n",
    "              torch.nn.Linear(D_in, H),\n",
    "              torch.nn.ReLU(),\n",
    "              torch.nn.Linear(H, D_out),\n",
    "            )\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    # Use the optim package to define an Optimizer that will update the weights of\n",
    "    # the model for us. Here we will use Adam; the optim package contains many other\n",
    "    # optimization algorithms. The first argument to the Adam constructor tells the\n",
    "    # optimizer which Tensors it should update.\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for t in range(500):\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        # Compute and print loss.\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        if t>498:\n",
    "            print(t, loss.item())\n",
    "            \n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the Tensors it will update (which are the learnable weights\n",
    "        # of the model)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Calling the step function on an Optimizer makes an update to its parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 3.5519506127457134e-06\n",
      "499 2.89068813330573e-09\n",
      "499 6.102735738977572e-08\n",
      "499 5.387914003840422e-10\n",
      "499 2.8562343601379325e-09\n",
      "499 1.3440622836924376e-08\n",
      "499 1.3865713022198634e-09\n",
      "499 6.465485036244445e-09\n",
      "1.7 s ± 96 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_sim_torchoptim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim_torchModule():\n",
    "    # Code in file nn/two_layer_net_module.py\n",
    "    import torch\n",
    "\n",
    "    class TwoLayerNet(torch.nn.Module):\n",
    "        def __init__(self, D_in, H, D_out):\n",
    "            \"\"\"\n",
    "            In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "            member variables.\n",
    "            \"\"\"\n",
    "            super(TwoLayerNet, self).__init__()\n",
    "            self.linear1 = torch.nn.Linear(D_in, H)\n",
    "            self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            In the forward function we accept a Tensor of input data and we must return\n",
    "            a Tensor of output data. We can use Modules defined in the constructor as\n",
    "            well as arbitrary (differentiable) operations on Tensors.\n",
    "            \"\"\"\n",
    "            h_relu = self.linear1(x).clamp(min=0)\n",
    "            y_pred = self.linear2(h_relu)\n",
    "            return y_pred\n",
    "\n",
    "    # N is batch size; D_in is input dimension;\n",
    "    # H is hidden dimension; D_out is output dimension.\n",
    "    N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "    # Create random Tensors to hold inputs and outputs\n",
    "    x = torch.randn(N, D_in)\n",
    "    y = torch.randn(N, D_out)\n",
    "\n",
    "    # Construct our model by instantiating the class defined above.\n",
    "    model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "    # Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "    # in the SGD constructor will contain the learnable parameters of the two\n",
    "    # nn.Linear modules which are members of the model.\n",
    "    loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "    for t in range(500):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        # Compute and print loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        if t>498:\n",
    "            print(t, loss.item())\n",
    "        \n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.5 ns ± 0.32 ns per loop (mean ± std. dev. of 7 runs, 100000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_sim_torchModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch API and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components\n",
    "\n",
    "The following are component of the API, and explained in-detail in the [docs](https://pytorch.org/docs/stable/nn.html#)\n",
    "\n",
    "* Linear layers - `nn.Linear`\n",
    "* Dropout layers - `nn.Dropout`\n",
    "* BatchNorm - `nn.BatchNorm1d`\n",
    "* Convolution layers - `nn.Conv1d`\n",
    "* Pooling layers - `n.MaxPool1d`\n",
    "* Padding layers - `ReflectionPad1d`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 30])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(in_features=20, out_features=30, bias=True)\n",
    "input = torch.randn(10, 20)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([3, 2, 4])\n",
      "tensor([[[-0.4115, -1.7695,  1.0313, -0.4850],\n",
      "         [ 0.4450,  1.0767,  0.7671,  1.1860]],\n",
      "\n",
      "        [[-0.6997, -1.2741,  0.5958,  0.6612],\n",
      "         [ 1.6600, -0.3820, -0.6644,  1.3638]],\n",
      "\n",
      "        [[ 1.8785, -0.5167,  1.2430,  1.5727],\n",
      "         [ 0.9379, -1.5978,  1.2981,  0.9275]]])\n",
      "3\n",
      "torch.Size([3, 1, 2])\n",
      "tensor([[[-0.2894, -0.1731]],\n",
      "\n",
      "        [[-1.0687,  0.5802]],\n",
      "\n",
      "        [[-0.0940, -0.4418]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(3, 2, 4)     #(samples, rows, columns/features)\n",
    "print(input.dim()); print(input.shape)\n",
    "print(input)\n",
    "m = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=1, stride=2)\n",
    "output = m(input)\n",
    "print(output.dim()); print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3767,  0.1609,  0.0140,  1.2529,  1.0479]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4709,  0.2012,  0.0176,  1.5661,  1.3098]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.2)    #p-probability of element being zero\n",
    "input = torch.randn(1, 5)\n",
    "print(input)\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2611, -0.6936,  0.9268,  0.5002, -0.5748]],\n",
       "\n",
       "        [[ 1.5226,  0.5925, -0.2946, -0.7670,  1.4274]],\n",
       "\n",
       "        [[-0.6122,  0.1138, -0.0378, -1.8526, -1.5118]]],\n",
       "       grad_fn=<NativeBatchNormBackward>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(3, 1, 5)\n",
    "m = nn.BatchNorm1d(num_features=1)\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5099,  0.5099]],\n",
       "\n",
       "        [[ 1.1373,  0.1945]],\n",
       "\n",
       "        [[ 0.9555, -0.3029]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.MaxPool1d(3, stride=2)    #pool of size=3, stride=2\n",
    "input = torch.randn(3, 1, 5)\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 2., 3.],\n",
      "         [4., 5., 6., 7.]]])\n",
      "tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],\n",
      "         [6., 5., 4., 5., 6., 7., 6., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.ReflectionPad1d(2)\n",
    "input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)\n",
    "print(input)\n",
    "print( m(input) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace following class code with an easy sequential network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = Net(1, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy and fast way to build your network\n",
    "net2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(10, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net1)     # net1 architecture\n",
    "print(net2)     # net2 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear layers expect the input to be represented in a 1-dimensional form. Thus we include a call to the “view” function, which converts the input from the 2- dimensional input into 1-dimension.  Note, PyTorch can still train in mini-batch mode. The view function converts the input tensor into the dimensions `[n,1,1,3072]`, where n is the mini-batch size.\n",
    "\n",
    "After passing data through our network we will have an output tensor of size `[n,1,1,10]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case: basic feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, sample_rate = librosa.load(file_path)\n",
    "  for ts in [0.75 ,1 ,1.25]: \n",
    "        for ps in [ −1 ,0 ,+1]:\n",
    "            samples new = librosa.effects.time_stretch(samples, rate=ts)\n",
    "            y_new = librosa.effects.pitch_shift(samples_new, sample_rate, n_steps=ps)\n",
    "\n",
    "max_length = 1.5 # Max length in seconds\n",
    "samples, sample rate = librosa.load(file_path)\n",
    "short_samples = librosa.util.fix_length(samples, sample_rate * max_length )\n",
    "melSpectrum = librosa.feature.melspectrogram(short_samples.astype(np.float16), sr=sample_rate , n_mels=128)\n",
    "logMelSpectrogram = librosa.power_to_db(melSpectrum, ref=np.max )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# PyTorch Network Definition\n",
    "class Model(nn.Module): \n",
    "    def init (self):\n",
    "        def super(Model, self).init() \n",
    "        self.fc1 = nn.Linear(3072, 128) \n",
    "        self.fc2 = nn.Linear(128, 128) \n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view((−1, 3072)) # Converts 2D data to 1D \n",
    "        h = self.fc1(x)\n",
    "        h=torch.relu(h)\n",
    "        h = self.fc2(h) \n",
    "        h=torch.relu(h)\n",
    "        h = self.fc3(h)\n",
    "        out = torch.log softmax(h,dim=1) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case: regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With deep learning we have multiple layers of computation with hidden values that are passed to subsequent layers. The output of each of these layers is likely to be a non-normalized input, and the distribution is likely to change frequently during the training process. This process is commonly referred to as “internal covariate shift.” Batch normalization [IS15] aims to reduce internal covariate shift in a network by normalizing the outputs of intermediate layers during training. This speeds the training process and allows for higher learning rates without risking divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the same using regularization techniques\n",
    "class Model(nn.Module): \n",
    "    def init (self):\n",
    "        super(Model, self).init() \n",
    "        self.fc1 = nn.Linear(3072, 128) \n",
    "        self.bc1 = nn.BatchNorm1d(128)   #Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) \n",
    "        self.fc2 = nn.Linear(128, 128) \n",
    "        self.bc2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view((−1, 3072))\n",
    "        h = self.fc1(x)\n",
    "        h=self.bc1(h)\n",
    "        h=torch.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training) #Disabled during evaluation\n",
    "        h = self.fc2(h)\n",
    "        h = self.bc2(h)\n",
    "        h = torch.relu(h)\n",
    "        h = F.dropout(h, p=0.2, training=self.training) #Disabled during evaluation\n",
    "        h = self.fc3(h)\n",
    "        out = torch.log_softmax(h,dim=1) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model( )\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters() , lr=0.01) \n",
    "n_epoch = 40\n",
    "for epoch in range(n epoch):\n",
    "    for data , target in train loader : \n",
    "        # Get Samples\n",
    "        if use cuda:\n",
    "            data , target = data . cuda () , target . cuda ()\n",
    "            # Clear gradients\n",
    "            optimizer . zero grad () # Forward Propagation\n",
    "            y pred = model(data) # Error Computation\n",
    "            loss = torch.cross entropy(y pred , target) # Backpropagation\n",
    "            loss.backward ()\n",
    "            # Parameter Update\n",
    "            optimizer.step()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case: autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This autoencoder learns a low-dimensional encoding of the input data that the decoder is able to produce examples.  The output of our network must be the same size as our input, d = 3072, thus the final layer of our network must ensure that the dimensionality matches the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch .nn. functional as F # In place operations for non−linearities\n",
    "\n",
    "# PyTorch Network Definition\n",
    "class autoencoder(nn.Module):\n",
    "    def init (self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.e_fc1 = nn.Linear(3072, 512)\n",
    "        self.e_fc2 = nn.Linear(512, 128)\n",
    "        self.e_fc3 = nn.Linear(128, 64)\n",
    "        self.e_fc4 = nn.Linear(64,64)\n",
    "        self.d_fc1 = nn.Linear(64, 64)\n",
    "        self.d_fc2 = nn.Linear(64, 128)\n",
    "        self.d_fc3 = nn.Linear(128, 512)\n",
    "        self.d_fc4 = nn.Linear(512, 3072)\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        h = F.relu(self.e_fc1(x))\n",
    "        h = F.relu(self.e_fc2(h))\n",
    "        h = F.relu(self.e_fc3(h))\n",
    "        h = self.e_fc4(h)\n",
    "        # Decoder\n",
    "        h = F.relu(self.d_fc1(h))\n",
    "        h = F.relu(self.d_fc2(h)) \n",
    "        h = F.relu(self.d_fc3(h)) \n",
    "        h = self.d_fc4(h)\n",
    "        out = F.tanh(h) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When examining the reconstructed inputs, we notice that they appear to be less sharp than the examples shown in Fig. 4.28. This is mainly due to the MSE loss function. Because it is computing the squared error, it tends to pull all values toward the mean prioritizing the average over specific areas of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "# Neural Network Training in PyTorch\n",
    "model = autoencoder() \n",
    "optimizer = optim.Adam(model.parameters() , lr=learning rate , weight_decay=1e−5)    #decay for regularization\n",
    "for epoch in range(n epoch):\n",
    "    for data, _ in train loader :\n",
    "        # Get samples\n",
    "        input = data.view(−1,3072)    # We will reuse the formatted input as our target \n",
    "        # Forward Propagation\n",
    "        output = model(input)     # Error Computation\n",
    "        loss = F.mse_loss(output, input)     # Clear gradients\n",
    "        optimizer.zero_grad()     # Backpropagation\n",
    "        loss.backward()\n",
    "        # Parameter Update\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "* [ref: stride](https://www.quora.com/What-does-stride-mean-in-the-context-of-convolutional-neural-networks)\n",
    "* [ref: pooling](https://www.quora.com/What-is-max-pooling-in-convolutional-neural-networks)\n",
    "* [ref: convolutional](https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/)\n",
    "* [ref: tutorials point](https://www.tutorialspoint.com/pytorch/pytorch_convolutional_neural_network.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long-Short Term Memory Model\n",
    "\n",
    "* [ref: math](https://medium.com/@aidangomez/let-s-do-this-f9b699de31d9)\n",
    "\n",
    "#### Comparison of LSTM Numpy (by-hand) and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create by-hand using numpy, [ref](https://towardsdatascience.com/the-lstm-reference-card-6163ca98ae87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by-hand\n",
    "import numpy as np \n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "def forget_gate(x, h, Weights_hf, Bias_hf, Weights_xf, Bias_xf, prev_cell_state):\n",
    "    forget_hidden  = np.dot(Weights_hf, h) + Bias_hf\n",
    "    forget_eventx  = np.dot(Weights_xf, x) + Bias_xf\n",
    "    return np.multiply( sigmoid(forget_hidden + forget_eventx), prev_cell_state )\n",
    "\n",
    "def input_gate(x, h, Weights_hi, Bias_hi, Weights_xi, Bias_xi, Weights_hl, Bias_hl, Weights_xl, Bias_xl):\n",
    "    ignore_hidden  = np.dot(Weights_hi, h) + Bias_hi\n",
    "    ignore_eventx  = np.dot(Weights_xi, x) + Bias_xi\n",
    "    learn_hidden   = np.dot(Weights_hl, h) + Bias_hl\n",
    "    learn_eventx   = np.dot(Weights_xl, x) + Bias_xl\n",
    "    return np.multiply( sigmoid(ignore_eventx + ignore_hidden), np.tanh(learn_eventx + learn_hidden) )\n",
    "\n",
    "\n",
    "def cell_state(forget_gate_output, input_gate_output):\n",
    "    return forget_gate_output + input_gate_output\n",
    "\n",
    "  \n",
    "def output_gate(x, h, Weights_ho, Bias_ho, Weights_xo, Bias_xo, cell_state):\n",
    "    out_hidden = np.dot(Weights_ho, h) + Bias_ho\n",
    "    out_eventx = np.dot(Weights_xo, x) + Bias_xo\n",
    "    return np.multiply( sigmoid(out_eventx + out_hidden), np.tanh(cell_state) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed final, fully-connected linear layer\n",
    "#set Parameters for a small LSTM network\n",
    "input_size  = 2 # size of one 'event', or sample, in our batch of data\n",
    "hidden_dim  = 3 # 3 cells in the LSTM layer\n",
    "output_size = 1 # desired model output\n",
    "\n",
    "def model_output(lstm_output, fc_Weight, fc_Bias):\n",
    "  '''Takes the LSTM output and transforms it to our desired \n",
    "  output size using a final, fully connected layer'''\n",
    "  return np.dot(fc_Weight, lstm_output) + fc_Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PyTorch LSTM with the same parameters. PyTorch will automatically assign the weights with random values — we’ll extract those and use them to initialize our NumPy network as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    " \n",
    "#Initialize an PyTorch LSTM for comparison to our Numpy LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        #LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        #Final, fully-connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = 1\n",
    "        # get LSTM outputs\n",
    "        lstm_output, (h,c) = self.lstm(x, hidden)\n",
    "        # shape output to be (batch_size*seq_length, hidden_dim)\n",
    "        lstm_output = lstm_output.view(-1, self.hidden_dim)  \n",
    "        # get final output \n",
    "        model_output = self.fc(lstm_output)\n",
    "        return model_output, (h,c)\n",
    "    \n",
    "torch.manual_seed(5)\n",
    "torch_lstm = LSTM(input_size = input_size, hidden_dim = hidden_dim, output_size = output_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('lstm.weight_ih_l0', tensor([[ 0.3813, -0.4317],\n",
      "        [ 0.4705,  0.3694],\n",
      "        [ 0.4851, -0.4427],\n",
      "        [-0.3875,  0.2747],\n",
      "        [-0.5389,  0.5706],\n",
      "        [ 0.1229,  0.0746],\n",
      "        [-0.4937,  0.1840],\n",
      "        [ 0.2483,  0.0916],\n",
      "        [ 0.5553,  0.1734],\n",
      "        [-0.5120,  0.4851],\n",
      "        [ 0.1960, -0.2754],\n",
      "        [-0.5303,  0.3291]])), ('lstm.weight_hh_l0', tensor([[ 0.5487, -0.4730,  0.0316],\n",
      "        [ 0.2071, -0.2726, -0.1263],\n",
      "        [-0.3855, -0.2730, -0.5264],\n",
      "        [-0.0134,  0.3423,  0.2808],\n",
      "        [ 0.5424, -0.5071, -0.0710],\n",
      "        [ 0.5621,  0.0945, -0.1628],\n",
      "        [-0.5200,  0.2687,  0.4383],\n",
      "        [ 0.4630,  0.4833,  0.1130],\n",
      "        [ 0.4115, -0.1453,  0.4689],\n",
      "        [-0.0494, -0.1191, -0.2870],\n",
      "        [ 0.3074,  0.2336,  0.3672],\n",
      "        [-0.3690, -0.3070,  0.5464]])), ('lstm.bias_ih_l0', tensor([-0.3205, -0.3293, -0.1545, -0.1866, -0.3926,  0.4666,  0.0644,  0.2632,\n",
      "         0.4282, -0.3741,  0.4407, -0.2892])), ('lstm.bias_hh_l0', tensor([-0.0919,  0.4369,  0.5323,  0.5068,  0.3320,  0.5366, -0.2080, -0.0367,\n",
      "        -0.1975, -0.0424, -0.0702,  0.3085])), ('fc.weight', tensor([[ 0.3968, -0.4158, -0.3188]])), ('fc.bias', tensor([-0.1776]))])\n"
     ]
    }
   ],
   "source": [
    "#extract and use weights with by-hand version\n",
    "state = torch_lstm.state_dict()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch documentation explains all we need to break this down:\n",
    "* The weights for each gate in are in this order: ignore, forget, learn, output\n",
    "* keys with ‘ih’ in the name are the weights/biases for the input, or Wx_ and Bx_\n",
    "* keys with ‘hh’ in the name are the weights/biases for the hidden state, or Wh_ and Bh_\n",
    "\n",
    "Given the parameters we chose, we can therefore extract the weights for the NumPy LSTM to use in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Event (x) Weights and Biases for all gates\n",
    "Weights_xi = state['lstm.weight_ih_l0'][0:3].numpy() # shape [h, x]\n",
    "Weights_xf = state['lstm.weight_ih_l0'][3:6].numpy() # shape [h, x]\n",
    "Weights_xl = state['lstm.weight_ih_l0'][6:9].numpy() # shape [h, x]\n",
    "Weights_xo = state['lstm.weight_ih_l0'][9:12].numpy() # shape [h, x]\n",
    " \n",
    "Bias_xi = state['lstm.bias_ih_l0'][0:3].numpy() #shape is [h, 1]\n",
    "Bias_xf = state['lstm.bias_ih_l0'][3:6].numpy() #shape is [h, 1]\n",
    "Bias_xl = state['lstm.bias_ih_l0'][6:9].numpy() #shape is [h, 1]\n",
    "Bias_xo = state['lstm.bias_ih_l0'][9:12].numpy() #shape is [h, 1]\n",
    " \n",
    "#Hidden state (h) Weights and Biases for all gates\n",
    "Weights_hi = state['lstm.weight_hh_l0'][0:3].numpy() #shape is [h, h]\n",
    "Weights_hf = state['lstm.weight_hh_l0'][3:6].numpy() #shape is [h, h]\n",
    "Weights_hl = state['lstm.weight_hh_l0'][6:9].numpy() #shape is [h, h]\n",
    "Weights_ho = state['lstm.weight_hh_l0'][9:12].numpy() #shape is [h, h]\n",
    " \n",
    "Bias_hi = state['lstm.bias_hh_l0'][0:3].numpy() #shape is [h, 1]\n",
    "Bias_hf = state['lstm.bias_hh_l0'][3:6].numpy() #shape is [h, 1]\n",
    "Bias_hl = state['lstm.bias_hh_l0'][6:9].numpy() #shape is [h, 1]\n",
    "Bias_ho = state['lstm.bias_hh_l0'][9:12].numpy() #shape is [h, 1]\n",
    " \n",
    "#--------------------------------------------------------------------\n",
    "# Final, fully connected layer Weights and Bias\n",
    "fc_Weight = state['fc.weight'][0].numpy() #shape is [h, output_size]\n",
    "fc_Bias = state['fc.bias'][0].numpy() #shape is [,output_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have two networks — one in PyTorch, one in NumPy — with access to the same starting weights. We’ll put some time series data through each to ensure they are identical. To do a forward pass with our network, we’ll pass the data into the LSTM gates in sequence, and print the output after each event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3479427319173535\n",
      "-0.47396493597023465\n",
      "-0.5263107365187176\n"
     ]
    }
   ],
   "source": [
    "#Simple Time Series Data\n",
    "data = np.array(\n",
    "[[1,1],\n",
    "[2,2],\n",
    "[3,3]])\n",
    " \n",
    "#Initialize cell and hidden states with zeroes\n",
    "h = np.zeros(hidden_dim)\n",
    "c = np.zeros(hidden_dim)\n",
    " \n",
    "#Loop through data, updating the hidden and cell states after each pass\n",
    "for eventx in data:\n",
    "    f = forget_gate(eventx, h, Weights_hf, Bias_hf, Weights_xf, Bias_xf, c)\n",
    "    i = input_gate(eventx, h, Weights_hi, Bias_hi, Weights_xi, Bias_xi, Weights_hl, Bias_hl, Weights_xl, Bias_xl)\n",
    "    c = cell_state(f,i)\n",
    "    h = output_gate(eventx, h, Weights_ho, Bias_ho, Weights_xo, Bias_xo, c)\n",
    "    print(model_output(h, fc_Weight, fc_Bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3479],\n",
      "        [-0.4740],\n",
      "        [-0.5263]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#rPyTorch expects an extra dimension for batch size:\n",
    "torch_batch = torch.Tensor(data).unsqueeze(0)\n",
    " \n",
    "torch_output, (torch_hidden, torch_cell) = torch_lstm(torch_batch, None)\n",
    "print(torch_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can additionally verify that after the data has gone through the LSTM cells, the two models have the same hidden and cell states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----------------------------------------\n",
      "Torch Hidden State: tensor([[[-0.1190,  0.4759,  0.3252]]], grad_fn=<StackBackward>)\n",
      "Torch Cell State: tensor([[[-0.3556,  1.1789,  1.3026]]], grad_fn=<StackBackward>)\n",
      "\n",
      "np Hidden State: [-0.11898849  0.47585365  0.32522364]\n",
      "np Cell State: [-0.3555854   1.17887101  1.3025983 ]\n"
     ]
    }
   ],
   "source": [
    "print('\\n','-'*40)\n",
    "print(f'Torch Hidden State: {torch_hidden}')\n",
    "print(f'Torch Cell State: {torch_cell}\\n')\n",
    "print(f'np Hidden State: {h}')\n",
    "print(f'np Cell State: {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning / Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Sentence Encoder (USE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XLNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialized Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Organizing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Random Field"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
