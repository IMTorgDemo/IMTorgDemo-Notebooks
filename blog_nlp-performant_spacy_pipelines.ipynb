{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b0b6012-6034-49d2-840c-510f558fd0f5",
   "metadata": {},
   "source": [
    "# Making SpaCy Pipelines Performant \n",
    "\n",
    "Date: 2021-01-01  \n",
    "Author: Jason Beach  \n",
    "Categories: Best_Practice, Introduction_Tutorial, Data_Science \n",
    "Tags: nlp, development, spacy\n",
    "\n",
    "<!--eofm-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbc20d3-5b37-432c-adc6-0ece9bb605c3",
   "metadata": {},
   "source": [
    "Grab the following:\n",
    "    * ~~create env with spacy2~~\n",
    "    * ~~use spacy2 and spacy3 in same notebook~~ => can use in same dir, but not same notebook\n",
    "    * performance\n",
    "    * sentence2vec, doc2vec\n",
    "    * ~~pattern-builder~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d62fd-e06a-4627-ab43-3a66fc56d65d",
   "metadata": {},
   "source": [
    "In this post we demonstrate the installation of spacy2 with important modules:\n",
    "\n",
    "* pattern-builder\n",
    "* neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d757c7b1-ddac-4b38-9852-fe04d9377cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print( python_version() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1332863d-a14b-4e75-bdca-16768c11d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ead4d0a-17b7-45d4-be77-76d1bc169bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b158f041-581c-4aaf-ba7b-b8d65ed01e36",
   "metadata": {},
   "source": [
    "## Linguistics for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae255a71-f479-43c4-ae8a-660ba6b30333",
   "metadata": {},
   "source": [
    "## Dependency Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04bc0b3-3922-450e-95be-b35ec865989c",
   "metadata": {},
   "source": [
    "## Pattern Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54105127-a5c8-483f-8341-03f292e2dfff",
   "metadata": {},
   "source": [
    "To install `pattern-builder` package try\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/sai-prasanna/spacy-pattern-builder.git\n",
    "cd spacy-pattern-builder\n",
    "pipenv install -r requirements.txt\n",
    "cd ..\n",
    "pipenv install -e spacy-pattern-builder/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c793195-8e09-4bf5-a3eb-51ebec05efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a SpaCy model, parse a string to create a Doc object\n",
    "import en_core_web_sm\n",
    "\n",
    "text = 'We introduce efficient methods for fitting Boolean models to molecular data.'\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(text)\n",
    "\n",
    "from spacy_pattern_builder import build_dependency_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "163e667a-cec9-4bed-b13e-6de00596e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a list of tokens we want to match.\n",
    "match_tokens = [doc[i] for i in [0, 1, 3]]  # [We, introduce, methods]\n",
    "\n",
    "''' Note that these tokens must be fully connected. That is,\n",
    "all tokens must have a path to all other tokens in the list,\n",
    "without needing to traverse tokens outside of the list.\n",
    "Otherwise, spacy-pattern-builder will raise a TokensNotFullyConnectedError.\n",
    "You can get a connected set that includes your tokens with the following: '''\n",
    "from spacy_pattern_builder import util\n",
    "connected_tokens = util.smallest_connected_subgraph(match_tokens, doc)\n",
    "assert match_tokens == connected_tokens  # In this case, the tokens we provided are already fully connected\n",
    "\n",
    "# Specify the token attributes / features to use\n",
    "feature_dict = {  # This is equal to the default feature_dict\n",
    "    'DEP': 'dep_',\n",
    "    'TAG': 'tag_'\n",
    "}\n",
    "\n",
    "# Build the pattern\n",
    "pattern = build_dependency_pattern(doc, match_tokens, feature_dict=feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df9b39e8-f820-4d5b-8fd3-154d4c322c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'PATTERN': {'DEP': 'ROOT', 'TAG': 'VBP'}, 'SPEC': {'NODE_NAME': 'node1'}},\n",
      " {'PATTERN': {'DEP': 'nsubj', 'TAG': 'PRP'},\n",
      "  'SPEC': {'NBOR_NAME': 'node1', 'NBOR_RELOP': '>', 'NODE_NAME': 'node0'}},\n",
      " {'PATTERN': {'DEP': 'dobj', 'TAG': 'NNS'},\n",
      "  'SPEC': {'NBOR_NAME': 'node0', 'NBOR_RELOP': '$--', 'NODE_NAME': 'node3'}}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n[{'PATTERN': {'DEP': 'ROOT', 'TAG': 'VBP'}, 'SPEC': {'NODE_NAME': 'node1'}},\\n {'PATTERN': {'DEP': 'nsubj', 'TAG': 'PRP'},\\n  'SPEC': {'NBOR_NAME': 'node1', 'NBOR_RELOP': '>', 'NODE_NAME': 'node0'}},\\n {'PATTERN': {'DEP': 'dobj', 'TAG': 'NNS'},\\n  'SPEC': {'NBOR_NAME': 'node1', 'NBOR_RELOP': '>', 'NODE_NAME': 'node3'}}]\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(pattern)  # In the format consumed by SpaCy's DependencyMatcher:\n",
    "'''\n",
    "[{'PATTERN': {'DEP': 'ROOT', 'TAG': 'VBP'}, 'SPEC': {'NODE_NAME': 'node1'}},\n",
    " {'PATTERN': {'DEP': 'nsubj', 'TAG': 'PRP'},\n",
    "  'SPEC': {'NBOR_NAME': 'node1', 'NBOR_RELOP': '>', 'NODE_NAME': 'node0'}},\n",
    " {'PATTERN': {'DEP': 'dobj', 'TAG': 'NNS'},\n",
    "  'SPEC': {'NBOR_NAME': 'node1', 'NBOR_RELOP': '>', 'NODE_NAME': 'node3'}}]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "700b8bf7-4f7a-4b8e-aef9-4b78ccebcf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match: 15329811787164753587\n",
      "token: [[1, 0, 3]]\n",
      "[We, introduce, methods]\n"
     ]
    }
   ],
   "source": [
    "# Create a matcher and add the newly generated pattern\n",
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "matcher = DependencyMatcher(doc.vocab)\n",
    "matcher.add('pattern', None, pattern)\n",
    "\n",
    "# And get matches\n",
    "matches = matcher(doc)\n",
    "for match_id, token_idxs in matches:\n",
    "    print(f'match: {match_id}')\n",
    "    print(f'token: {token_idxs}')\n",
    "    tokens = [doc[i] for i in token_idxs[0]]    #<<<KEY_CHANGE: token_idxs is now a list, so another loop is needed if there is more than one item\n",
    "    tokens = sorted(tokens, key=lambda w: w.i)  # Make sure tokens are in their original order\n",
    "    print(tokens)  # [We, introduce, methods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "966465d5-1b92-41b6-858d-21c8b81f307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'We introduce a slightly different but still efficient method for this test.'\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(text)\n",
    "\n",
    "# And get matches\n",
    "matches = matcher(doc)\n",
    "for match_id, token_idxs in matches:\n",
    "    print(f'match: {match_id}')\n",
    "    print(f'token: {token_idxs}')\n",
    "    tokens = [doc[i] for i in token_idxs[0]]    #<<<KEY_CHANGE: token_idxs is now a list, so another loop is needed if there is more than one item\n",
    "    tokens = sorted(tokens, key=lambda w: w.i)  # Make sure tokens are in their original order\n",
    "    print(tokens)  # [We, introduce, methods]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d090118-26ec-439d-be2e-002be0d373b7",
   "metadata": {},
   "source": [
    "*** I THINK THIS^^^ IS STILL BROKEN ****\n",
    "\n",
    "TAKE A LOOK AT THE EARLIER FIX: https://github.com/cyclecycle/spacy-pattern-builder/pull/2/commits/3b430d9f78a52117af86d12797bd8e2ee02fc0fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58e7e3-4488-4051-8299-31e51615a7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770bd9f-6dd1-4993-9e9a-59222a10eda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61450309-cf83-4c26-af2d-5695e833c392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2adf585-cae8-4cba-8979-98da9bea50d2",
   "metadata": {},
   "source": [
    "## NeuralCoref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77280559-1b31-4e12-a386-9015d6713a72",
   "metadata": {},
   "source": [
    "To install `neuralcoref` package try\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/huggingface/neuralcoref.git\n",
    "cd neuralcoref\n",
    "pip install -r requirements.txt\n",
    "cd ..\n",
    "pip install -e neuralcoref/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f555323c-08aa-49da-a320-b43ab105e89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[My sister: [My sister, She], a dog: [a dog, him]]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your usual SpaCy model (one of SpaCy English models)\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add neural coref to SpaCy's pipe\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# You're done. You can now use NeuralCoref as you usually manipulate a SpaCy document annotations.\n",
    "doc = nlp(u'My sister has a dog. She loves him.')\n",
    "\n",
    "doc._.has_coref\n",
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b503d-8207-48c0-a70f-992f70443746",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8a031d9-50dd-4ba6-826f-6bdcbe947607",
   "metadata": {},
   "source": [
    "## Performance Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f1c40-afb3-4dcd-8e97-d148a4764111",
   "metadata": {},
   "source": [
    "* `%time`: time the execution of a single statement\n",
    "* `%timeit`: time repeated execution of a single statement for more accuracy; %%timeit -n1 -r1 => %time\n",
    "* `%prun`: run code wtih the profiler\n",
    "* `%lprun`: run code with the line-by-line profiler\n",
    "* `%memit`: measure the memory use of a single statement\n",
    "* `%mprun`: run code with the line-by-line memory profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0093d221-a454-478c-9bae-4fef45568b96",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8f15614-40e8-46ed-b52f-2291eb0f6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e955a2f9-dcea-42e5-9915-91296ccebf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './resources/Test_Performance'\n",
    "file_name = 'test_file'\n",
    "file_path = os.path.join(dir_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b26ff3-da82-4051-8183-4ecd553ab7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6534be3c-cb71-45c3-8bc4-cefd4ff0b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'w') as file:\n",
    "    idx = 0\n",
    "    for idx in range(100):\n",
    "        file.writelines(f'This is sentence {idx+1} used for testing our NLP pipeline. ' * 10 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02aba339-93a7-4268-af8f-66e1a36704eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_file\n"
     ]
    }
   ],
   "source": [
    "! ls resources/Test_Performance/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41a73e-0f74-4382-9ccb-d9a74edc66ad",
   "metadata": {},
   "source": [
    "### Methods to read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd778b5e-299a-4cb1-9317-ac906402cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive\n",
    "def iter_text_lines(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            yield line\n",
    "        \n",
    "iter = iter_text_lines(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c49e9273-0ccd-4fd3-8bbf-a1b853a9377c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. \\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "209d74e9-e1bf-454f-b347-849cef22bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch lines returned\n",
    "def iter_text_batch(file_object, batch_size=10):\n",
    "    \"\"\"Lazy function (generator) to read a file piece by piece\n",
    "    Default chunk size: 10.\"\"\"\n",
    "    while True:\n",
    "        data = []\n",
    "        for idx in range(batch_size):\n",
    "            data.append(file_object.readline())\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n",
    "        \n",
    "file_obj = open(file_path)\n",
    "iter = iter_text_batch(file_obj, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16d62352-c9e0-4c5a-ac3a-b03dbf27f13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. \\n',\n",
       " 'This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. \\n',\n",
       " 'This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. \\n']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd8b18a0-73e3-4516-b4bc-306b5f952f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bcf6cad0-4d81-4647-89c9-9080023c0bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. \\n', 'This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. This is sentence 2 used for testing our NLP pipeline. \\n', 'This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. This is sentence 3 used for testing our NLP pipeline. \\n', 'This is sentence 4 used for testing our NLP pipeline. This is sentence 4 used for testing our NLP pipeline. This is sentence 4 used for testing our NLP pipeline. This is sentence 4 used for testing our NLP pipeline. This is sentence 4 used for testing our NLP pipeline. This is sentence 4 used for testing our NLP pipeline. This is sentence 4 used for testing our NLP pipeline. This is sentence 4 used for testing our NLP pipeline. This is sentence 4 used for testing our NLP pipeline. This is sentence 4 used for testing our NLP pipeline. \\n', 'This is sentence 5 used for testing our NLP pipeline. This is sentence 5 used for testing our NLP pipeline. This is sentence 5 used for testing our NLP pipeline. This is sentence 5 used for testing our NLP pipeline. This is sentence 5 used for testing our NLP pipeline. This is sentence 5 used for testing our NLP pipeline. This is sentence 5 used for testing our NLP pipeline. This is sentence 5 used for testing our NLP pipeline. This is sentence 5 used for testing our NLP pipeline. This is sentence 5 used for testing our NLP pipeline. \\n', 'This is sentence 6 used for testing our NLP pipeline. This is sentence 6 used for testing our NLP pipeline. This is sentence 6 used for testing our NLP pipeline. This is sentence 6 used for testing our NLP pipeline. This is sentence 6 used for testing our NLP pipeline. This is sentence 6 used for testing our NLP pipeline. This is sentence 6 used for testing our NLP pipeline. This is sentence 6 used for testing our NLP pipeline. This is sentence 6 used for testing our NLP pipeline. This is sentence 6 used for testing our NLP pipeline. \\n', 'This is sentence 7 used for testing our NLP pipeline. This is sentence 7 used for testing our NLP pipeline. This is sentence 7 used for testing our NLP pipeline. This is sentence 7 used for testing our NLP pipeline. This is sentence 7 used for testing our NLP pipeline. This is sentence 7 used for testing our NLP pipeline. This is sentence 7 used for testing our NLP pipeline. This is sentence 7 used for testing our NLP pipeline. This is sentence 7 used for testing our NLP pipeline. This is sentence 7 used for testing our NLP pipeline. \\n', 'This is sentence 8 used for testing our NLP pipeline. This is sentence 8 used for testing our NLP pipeline. This is sentence 8 used for testing our NLP pipeline. This is sentence 8 used for testing our NLP pipeline. This is sentence 8 used for testing our NLP pipeline. This is sentence 8 used for testing our NLP pipeline. This is sentence 8 used for testing our NLP pipeline. This is sentence 8 used for testing our NLP pipeline. This is sentence 8 used for testing our NLP pipeline. This is sentence 8 used for testing our NLP pipeline. \\n', 'This is sentence 9 used for testing our NLP pipeline. This is sentence 9 used for testing our NLP pipeline. This is sentence 9 used for testing our NLP pipeline. This is sentence 9 used for testing our NLP pipeline. This is sentence 9 used for testing our NLP pipeline. This is sentence 9 used for testing our NLP pipeline. This is sentence 9 used for testing our NLP pipeline. This is sentence 9 used for testing our NLP pipeline. This is sentence 9 used for testing our NLP pipeline. This is sentence 9 used for testing our NLP pipeline. \\n', 'This is sentence 10 used for testing our NLP pipeline. This is sentence 10 used for testing our NLP pipeline. This is sentence 10 used for testing our NLP pipeline. This is sentence 10 used for testing our NLP pipeline. This is sentence 10 used for testing our NLP pipeline. This is sentence 10 used for testing our NLP pipeline. This is sentence 10 used for testing our NLP pipeline. This is sentence 10 used for testing our NLP pipeline. This is sentence 10 used for testing our NLP pipeline. This is sentence 10 used for testing our NLP pipeline. \\n']\n",
      "['This is sentence 11 used for testing our NLP pipeline. This is sentence 11 used for testing our NLP pipeline. This is sentence 11 used for testing our NLP pipeline. This is sentence 11 used for testing our NLP pipeline. This is sentence 11 used for testing our NLP pipeline. This is sentence 11 used for testing our NLP pipeline. This is sentence 11 used for testing our NLP pipeline. This is sentence 11 used for testing our NLP pipeline. This is sentence 11 used for testing our NLP pipeline. This is sentence 11 used for testing our NLP pipeline. \\n', 'This is sentence 12 used for testing our NLP pipeline. This is sentence 12 used for testing our NLP pipeline. This is sentence 12 used for testing our NLP pipeline. This is sentence 12 used for testing our NLP pipeline. This is sentence 12 used for testing our NLP pipeline. This is sentence 12 used for testing our NLP pipeline. This is sentence 12 used for testing our NLP pipeline. This is sentence 12 used for testing our NLP pipeline. This is sentence 12 used for testing our NLP pipeline. This is sentence 12 used for testing our NLP pipeline. \\n', 'This is sentence 13 used for testing our NLP pipeline. This is sentence 13 used for testing our NLP pipeline. This is sentence 13 used for testing our NLP pipeline. This is sentence 13 used for testing our NLP pipeline. This is sentence 13 used for testing our NLP pipeline. This is sentence 13 used for testing our NLP pipeline. This is sentence 13 used for testing our NLP pipeline. This is sentence 13 used for testing our NLP pipeline. This is sentence 13 used for testing our NLP pipeline. This is sentence 13 used for testing our NLP pipeline. \\n', 'This is sentence 14 used for testing our NLP pipeline. This is sentence 14 used for testing our NLP pipeline. This is sentence 14 used for testing our NLP pipeline. This is sentence 14 used for testing our NLP pipeline. This is sentence 14 used for testing our NLP pipeline. This is sentence 14 used for testing our NLP pipeline. This is sentence 14 used for testing our NLP pipeline. This is sentence 14 used for testing our NLP pipeline. This is sentence 14 used for testing our NLP pipeline. This is sentence 14 used for testing our NLP pipeline. \\n', 'This is sentence 15 used for testing our NLP pipeline. This is sentence 15 used for testing our NLP pipeline. This is sentence 15 used for testing our NLP pipeline. This is sentence 15 used for testing our NLP pipeline. This is sentence 15 used for testing our NLP pipeline. This is sentence 15 used for testing our NLP pipeline. This is sentence 15 used for testing our NLP pipeline. This is sentence 15 used for testing our NLP pipeline. This is sentence 15 used for testing our NLP pipeline. This is sentence 15 used for testing our NLP pipeline. \\n', 'This is sentence 16 used for testing our NLP pipeline. This is sentence 16 used for testing our NLP pipeline. This is sentence 16 used for testing our NLP pipeline. This is sentence 16 used for testing our NLP pipeline. This is sentence 16 used for testing our NLP pipeline. This is sentence 16 used for testing our NLP pipeline. This is sentence 16 used for testing our NLP pipeline. This is sentence 16 used for testing our NLP pipeline. This is sentence 16 used for testing our NLP pipeline. This is sentence 16 used for testing our NLP pipeline. \\n', 'This is sentence 17 used for testing our NLP pipeline. This is sentence 17 used for testing our NLP pipeline. This is sentence 17 used for testing our NLP pipeline. This is sentence 17 used for testing our NLP pipeline. This is sentence 17 used for testing our NLP pipeline. This is sentence 17 used for testing our NLP pipeline. This is sentence 17 used for testing our NLP pipeline. This is sentence 17 used for testing our NLP pipeline. This is sentence 17 used for testing our NLP pipeline. This is sentence 17 used for testing our NLP pipeline. \\n', 'This is sentence 18 used for testing our NLP pipeline. This is sentence 18 used for testing our NLP pipeline. This is sentence 18 used for testing our NLP pipeline. This is sentence 18 used for testing our NLP pipeline. This is sentence 18 used for testing our NLP pipeline. This is sentence 18 used for testing our NLP pipeline. This is sentence 18 used for testing our NLP pipeline. This is sentence 18 used for testing our NLP pipeline. This is sentence 18 used for testing our NLP pipeline. This is sentence 18 used for testing our NLP pipeline. \\n', 'This is sentence 19 used for testing our NLP pipeline. This is sentence 19 used for testing our NLP pipeline. This is sentence 19 used for testing our NLP pipeline. This is sentence 19 used for testing our NLP pipeline. This is sentence 19 used for testing our NLP pipeline. This is sentence 19 used for testing our NLP pipeline. This is sentence 19 used for testing our NLP pipeline. This is sentence 19 used for testing our NLP pipeline. This is sentence 19 used for testing our NLP pipeline. This is sentence 19 used for testing our NLP pipeline. \\n', 'This is sentence 20 used for testing our NLP pipeline. This is sentence 20 used for testing our NLP pipeline. This is sentence 20 used for testing our NLP pipeline. This is sentence 20 used for testing our NLP pipeline. This is sentence 20 used for testing our NLP pipeline. This is sentence 20 used for testing our NLP pipeline. This is sentence 20 used for testing our NLP pipeline. This is sentence 20 used for testing our NLP pipeline. This is sentence 20 used for testing our NLP pipeline. This is sentence 20 used for testing our NLP pipeline. \\n']\n"
     ]
    }
   ],
   "source": [
    "LIMIT = 2\n",
    "with open(file_path) as file:\n",
    "    for idx, batch in enumerate(iter_text_batch(file)):\n",
    "        print(batch)\n",
    "        if (idx + 1) >= LIMIT:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e0c2ff1f-e3a9-4dbd-9be9-f847e7b58daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text_chunk(file_object, chunksize):\n",
    "    \"\"\"Lazy function (generator) to read a file piece by piece\n",
    "    Default chunksize: 10\"\"\"\n",
    "    while True:\n",
    "        for idx in range(chunksize):\n",
    "            data = file_object.readline()\n",
    "            if not data:\n",
    "                break\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "614beddb-d302-4f5f-a76a-4d83e9e1eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class iter_chunk_strm(object):\n",
    "    \n",
    "    def __init__(self, file_path, number_of_chunks, chunksize):\n",
    "        self.file_path = file_path\n",
    "        self.number_of_chunks = number_of_chunks\n",
    "        self.chunksize = chunksize\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with open(self.file_path) as file:\n",
    "            for chunk_idx in range(self.number_of_chunks):\n",
    "                for idx, batch in enumerate( gen_text_chunk(file, self.chunksize) ):\n",
    "                    yield batch\n",
    "                    if (idx + 1) >= self.chunksize:\n",
    "                        break\n",
    "                        \n",
    "iterator = (iter_chunk_strm(file_path, 2, chunksize=3)).__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de1d76ea-913a-40c0-9870-b73e12f6598e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. This is sentence 1 used for testing our NLP pipeline. \\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next( iterator )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a1c28-de61-4ce0-ab70-9e6280906dc6",
   "metadata": {},
   "source": [
    "### Test methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6195d13c-aaf0-4c66-a946-ce03a277409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a5adf-319a-41a6-b1fb-97481090cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"input_component\", last=True)\n",
    "nlp.add_pipe(\"mdl_large_fasttext\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd86ac-24f2-4936-8417-e759b078a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "result = []\n",
    "iter = iter_chunk_strm(file_path, 1, chunksize=1000)    #next( iter.__iter__() )\n",
    "#next( nlp_spacy.pipe(iter) )\n",
    "for doc in nlp_spacy.pipe(iter, n_process=1, batch_size=1000):\n",
    "    result.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac46caf-fe06-4314-afb4-8e3e06c38f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "BATCH = 10\n",
    "result = []\n",
    "#iter = iter_text_lines(file_path)\n",
    "for idx, doc in enumerate( nlp_spacy.pipe(iter, n_process=1, batch_size=BATCH) ):\n",
    "    result.append(doc)\n",
    "    if idx > BATCH:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5152814-f2ee-4fb1-8fb3-b9dab18e2b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "BATCH = 1000\n",
    "result = []\n",
    "#iter = iter_text_lines(file_path)\n",
    "for idx, doc in enumerate( nlp_spacy.pipe(my_array[0:1000], n_process=1, batch_size=BATCH) ):\n",
    "    result.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bff8143a-6cd1-4d04-8997-01619ea99db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rmdir(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27735fad-5c87-48b4-89f2-544acfccb549",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [iterators for data processing](https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/)\n",
    "* [inside spacy's pipe method](https://explosion.ai/blog/multithreading-with-cython)\n",
    "* [3 approaches to improving spacy performance](https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html) \n",
    "* [using spacy with spark distributed processing](https://haridas.in/run-spacy-jobs-on-apache-spark.html)\n",
    "* [different python profilers](https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782067bc-df27-4865-9d8a-95280e41b058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy2",
   "language": "python",
   "name": "spacy2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
